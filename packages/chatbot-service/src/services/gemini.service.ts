import { GoogleGenerativeAI, GenerativeModel, Content } from '@google/generative-ai';
import { MensagemHistorico, RespostaIA, DadosColetados } from '@cm/shared';

export class GeminiService {
    private genAI: GoogleGenerativeAI | null = null;
    private model: GenerativeModel | null = null;

    constructor() {
        const apiKey = process.env.GEMINI_API_KEY;
        if (!apiKey) {
            console.warn('‚ö†Ô∏è GEMINI_API_KEY n√£o configurada - respostas de fallback ser√£o usadas');
            return;
        }

        this.genAI = new GoogleGenerativeAI(apiKey);
        this.model = this.genAI.getGenerativeModel({
            model: 'gemini-1.5-flash',
            generationConfig: {
                temperature: 0.7,
                topP: 0.9,
                maxOutputTokens: 500,
            }
        });
        console.log('‚úÖ Gemini Service inicializado');
    }

    async gerarResposta(
        promptSistema: string,
        historico: MensagemHistorico[],
        mensagemAtual: string
    ): Promise<RespostaIA> {
        // Se n√£o tem model configurado, retornar fallback
        if (!this.model) {
            console.warn('‚ö†Ô∏è Gemini n√£o dispon√≠vel - usando resposta de fallback');
            return {
                mensagem: 'Ol√°! Obrigada pelo contato com a CM Modulados! üíú Em breve um atendente ir√° te ajudar.',
                avancar_etapa: false,
                dados_extraidos: {},
                finalizar_atendimento: false,
                enviar_para_humano: true
            };
        }

        try {
            // Construir hist√≥rico de conversa
            const historicoFormatado = historico.slice(-10).map(msg => ({
                role: msg.papel === 'cliente' ? 'user' : 'model',
                parts: [{ text: msg.conteudo }]
            })) as Content[];

            // Prompt com instru√ß√µes de resposta estruturada
            const promptCompleto = `${promptSistema}

INSTRU√á√ïES DE RESPOSTA:
Voc√™ deve responder APENAS com um JSON v√°lido no seguinte formato:
{
  "mensagem": "sua resposta humanizada aqui",
  "avancar_etapa": true/false,
  "dados_extraidos": { "campo": "valor" },
  "finalizar_atendimento": true/false,
  "enviar_para_humano": true/false
}

REGRAS DO JSON:
- "mensagem": texto da resposta para o cliente (humanizada, com emojis moderados)
- "avancar_etapa": true apenas se TODOS os requisitos da etapa foram cumpridos
- "dados_extraidos": extraia informa√ß√µes mencionadas (nome_completo, ambiente_interesse, quer_agendar_visita, etc)
- "finalizar_atendimento": true apenas na etapa de finaliza√ß√£o quando tudo estiver completo
- "enviar_para_humano": true se o cliente pedir para falar com humano ou assunto muito complexo

Mensagem atual do cliente: "${mensagemAtual}"

Responda APENAS com o JSON, sem markdown ou explica√ß√µes.`;

            // Iniciar chat com hist√≥rico
            const chat = this.model.startChat({
                history: historicoFormatado,
            });

            const result = await chat.sendMessage(promptCompleto);
            const respostaTexto = result.response.text();

            // Tentar parsear JSON
            try {
                // Limpar resposta (remover markdown se houver)
                let jsonLimpo = respostaTexto
                    .replace(/```json\n?/g, '')
                    .replace(/```\n?/g, '')
                    .trim();

                const resposta: RespostaIA = JSON.parse(jsonLimpo);

                return {
                    mensagem: resposta.mensagem || 'Desculpe, n√£o entendi. Pode repetir?',
                    avancar_etapa: resposta.avancar_etapa || false,
                    dados_extraidos: resposta.dados_extraidos || {},
                    finalizar_atendimento: resposta.finalizar_atendimento || false,
                    enviar_para_humano: resposta.enviar_para_humano || false
                };

            } catch (parseError) {
                console.error('Erro ao parsear resposta do Gemini:', parseError);
                console.log('Resposta bruta:', respostaTexto);

                // Fallback: usar texto bruto como mensagem
                return {
                    mensagem: respostaTexto.substring(0, 500),
                    avancar_etapa: false,
                    dados_extraidos: {},
                    finalizar_atendimento: false,
                    enviar_para_humano: false
                };
            }

        } catch (error) {
            console.error('Erro ao chamar Gemini:', error);

            // Resposta de fallback
            return {
                mensagem: 'Desculpe, estou com uma dificuldade t√©cnica. Em instantes retorno! üòä',
                avancar_etapa: false,
                dados_extraidos: {},
                finalizar_atendimento: false,
                enviar_para_humano: true
            };
        }
    }
}
